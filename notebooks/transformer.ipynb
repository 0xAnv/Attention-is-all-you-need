{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf821f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "# OR , \n",
    "# 80% max prealloc below \n",
    "# %env XLA_PYTHON_CLIENT_MEM_FRACTION=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aadc2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'   # or 'False' â€” both work\n",
    "# os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = str(0.8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dca3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx as nn\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    RTX 3060 optimised multi-head self attention module.\n",
    "    Works best with ampere gpus \n",
    "    Functionality:\n",
    "        - Uses fused QKV projections for better performance \n",
    "        - Has mixed precision softmax for improved numerical stability\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model:int, \n",
    "                 num_heads:int, \n",
    "                 rngs:nn.Rngs, \n",
    "                 dtype:jnp.dtype=jnp.bfloat16 ):\n",
    "        super().__init__()\n",
    "        # ensure tensor dimensions align properly \n",
    "        if d_model % num_heads != 0: raise ValueError(f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\")\n",
    "        self.d_model = d_model \n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.dtype=dtype\n",
    "        \n",
    "        # scale factor for dot product attention : scale = 1/sqrt(d_k) \n",
    "        # normalises the variance of the dot product to prevent it from growing too large,\n",
    "        # which can lead to vanishing gradients during training\n",
    "        self.scale = jax.lax.rsqrt(jnp.array(self.d_k, dtype=self.dtype))\n",
    "\n",
    "        # FUSED PROJECTION: Computes Q, K, V in single matrix multiplication \n",
    "        # matrix shape: (d_model, 3 * d_model) to produce concatenated QKV\n",
    "        # this is more efficient than separate projections for Q, K, V , \n",
    "        # since we can fuse a single GEMM operation which is highly optimized on modern hardware\n",
    "        self.qkv_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=3 * self.d_model,\n",
    "            use_bias=False, # Bias in QKV is generally unnecessary and slows down compute\n",
    "            dtype=self.dtype,\n",
    "            param_dtype=self.dtype, \n",
    "            rngs=rngs\n",
    "        )   \n",
    "\n",
    "        # output projection to combine multi-head outputs back to d_model dimensions\n",
    "        self.out_proj = nn.Linear(\n",
    "            in_features=self.d_model, \n",
    "            out_features=self.d_model, \n",
    "            use_bias=False,\n",
    "            dtype=self.dtype,\n",
    "            param_dtype=self.dtype,\n",
    "            rngs=rngs\n",
    "        )\n",
    "\n",
    "    # forward pass for multi-head self attention\n",
    "    def __call__(self, x:jax.Array, mask:jax.Array|None=None) -> jax.Array:\n",
    "        \"\"\"\n",
    "        x: Input tensor of shape (batch_size, seq_length, d_model)\n",
    "        mask: Optional boolean mask of shape (batch_size, 1, seq_length, seq_length) \n",
    "              for masking out padding tokens or future tokens in decoder\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        assert D == self.d_model, f\"Input feature dimension ({D}) must match model dimension ({self.d_model})\"\n",
    "\n",
    "        # fused projection \n",
    "        qkv = self.qkv_proj(x) # shape: (B, L, 3 * d_model)\n",
    "\n",
    "        # reshape and split into Q, K, V\n",
    "        # Target shape before split : (B, L, 3, num_heads, d_k)\n",
    "        qkv = qkv.reshape(B, L, 3, self.num_heads, self.d_k) \n",
    "\n",
    "        # transpose to isolate Q, K, V across 3rd dimension \n",
    "        # shape : (3, B, num_heads, L, d_k)\n",
    "        qkv = jnp.transpose(qkv, (2,0,3,1,4)) # shape : (3, B, num_heads, L, d_K)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] \n",
    "\n",
    "        # Scaled dot product attention \n",
    "        # Einsum handles the batched multi head multiplication\n",
    "        # Q @ K^T. \n",
    "        logits = jnp.einsum('bhqd, bhkd -> bhqk', q, k) * self.scale\n",
    "\n",
    "        # apply mask if provided, casual masking for decoder \n",
    "        if mask is not None:\n",
    "            # we use a very large negative number to drive the softmax probability to zero \n",
    "            # dtype must match logits \n",
    "            neg_inf = jnp.array(-1e9, dtype=logits.dtype)\n",
    "            logits = jnp.where(mask, logits, neg_inf)\n",
    "        \n",
    "        # for numerical stability we cast to fp32 for softmax \n",
    "        # then back to bfloat16 \n",
    "        attention_weights = jax.nn.softmax(logits.astype(jnp.float32), axis=-1).astype(self.dtype)\n",
    "\n",
    "        # multiplying attention weights to V \n",
    "        # v : (B, num_heads, L, d_k) \n",
    "        # attention_weights : (B, num_heads, L, L)\n",
    "        # attention_weights @ v^T shape: (B, num_heads, L, d_k) \n",
    "        context = jnp.einsum('bhqk, bhkd -> bhqd',attention_weights, v) \n",
    "\n",
    "        # recombining heads and flattening the last two dimensions \n",
    "        context = jnp.transpose(context, (0,2,1,3)).reshape(B, L, self.d_model)\n",
    "\n",
    "        # final linear projections to get maximum context\n",
    "        return self.out_proj(context) \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e578de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out Multi head attention \n",
    "def test_mha():\n",
    "    # static architecture parameters \n",
    "    B = 32 # batch_size, \n",
    "    L = 128 # max_seq_len \n",
    "    d_model=512 # paper replica\n",
    "    n_heads=8  # paper replica \n",
    "\n",
    "    # random state maneger \n",
    "    rng = nn.Rngs(744)\n",
    "\n",
    "    # initiate the model \n",
    "    mha = MultiHeadSelfAttention(\n",
    "        d_model=d_model, \n",
    "        num_heads=n_heads, \n",
    "        rngs=rng,\n",
    "        dtype=jnp.bfloat16 # Maximize RTX 3060 throughput\n",
    "    )\n",
    "\n",
    "    # dummy data simulating embedding layer output \n",
    "    key = jax.random.PRNGKey(seed=0) \n",
    "    dummy_x = jax.random.normal(key, (B, L, d_model), dtype=jnp.bfloat16) \n",
    "\n",
    "    # dummy mask to test decoder attention\n",
    "    # Brodcast across batch and heads \n",
    "    # Lower triangular matrix of boolean, (true allows attention , false masks it) \n",
    "    # shape : (1, 1, L, L) \n",
    "    causal_mask = jnp.tril(jnp.ones((1, 1, L, L), dtype=jnp.bool_))\n",
    "\n",
    "    # forward pass \n",
    "    print(\"--- Multi-Head Attention Forward Pass ---\")\n",
    "    print(f\"Input shape (B, L, d_model): {dummy_x.shape} | dtype: {dummy_x.dtype}\")\n",
    "    print(f\"Mask shape: {causal_mask.shape}\")\n",
    "    output = mha(dummy_x, mask=causal_mask)\n",
    "    \n",
    "    print(f\"Output shape (B, L, d_model): {output.shape} | dtype: {output.dtype}\")\n",
    "    print(f\"Output device: {output.device}\")\n",
    "    \n",
    "    # Assertions to ensure strict mathematical invariants\n",
    "    assert output.shape == dummy_x.shape, \"Output geometry must match input geometry\"\n",
    "    assert output.dtype == jnp.bfloat16, \"Precision leaked during compute\"\n",
    "    print(\"All architectural invariants passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ca8d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Multi-Head Attention Forward Pass ---\n",
      "Input shape (B, L, d_model): (32, 128, 512) | dtype: bfloat16\n",
      "Mask shape: (1, 1, 128, 128)\n",
      "Output shape (B, L, d_model): (32, 128, 512) | dtype: bfloat16\n",
      "Output device: cuda:0\n",
      "All architectural invariants passed.\n"
     ]
    }
   ],
   "source": [
    "test_mha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37567f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
